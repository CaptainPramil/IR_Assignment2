# -*- coding: utf-8 -*-
"""new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Of_iSPp2sSsJnlrkYYBOMY_NqJcTRyll
"""

#!pip install nltk
#!pip install pandas
#!pip install numpy
#import nltk
#nltk.download('stopwords')
#nltk.download('punkt')
#!pip install ipython-autotime

#%load_ext autotime

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

import nltk
import os
import string
import numpy as np
import copy
import pandas as pd
import pickle





"""# Data Extraction"""

title = "H"
#print(title)

paths = []
for (dirpath, dirnames, filenames) in os.walk("/content/H"):
    for i in filenames:
        paths.append(str(dirpath)+str("/")+i)
#for (dirpath, dirnames, filenames) in os.walk(str(os.getcwd())+'/'+title+'/rec.motorcycles)'):
    #for i in filenames:
        #paths.append(str(dirpath)+str("/")+i)

#paths

# paths.remove(paths[0]) #my computer is generating a dummy file, removing that!

#len(paths)

#def jaccard_similarity(query, document):
    
    #intersection = set(query).intersection(set(document))
    #union = set(query).union(set(document))
    #return len(intersection)/len(union)



"""# Preprocessing"""

def remove_header(datta):
    try:
        ind = datta.index('\n\n')
        try:
          tata=datta[ind:]
          datta = tata
        except:
          pass 
    except:
      pass
    return datta

def lowercase(datta):
    a=np.char.lower(datta)
    return a

def stopwords(datta):
    stop_words = stopwords.words('english')
    strdata=str(datta)
    strdata = word_tokenize(strdata)
    new_text = ""
    for w in strdata:
        if w not in stop_words:
            new_text = new_text + " "
            new_text=new_text+w
    retu=np.char.strip(new_text)
    return retu

def punctuation(datta):
    symbols = "!\"#$%&()*+-./"
    extra=":;<=>?@[\]^_`{|}~\n"
    symbols=symbols+extra
    leng=len(symbols)
    for i in range(leng):
        temp=symbols[i]
        tempspace=' '
        datta = np.char.replace(datta, temp, tempspace)
        space="  "
        againspace=" "
        datta = np.char.replace(datta, space, againspace)
    
    return np.char.replace(datta, ',', '')

def apostrophe(datta):
    retu=np.char.replace(datta, "'", "")
    return retu

def singlecharacters(datta):
    words = word_tokenize(str(datta))
    new_text = ""
    for wo in words:
        x=len(wo)
        if x > 1:
            new_text = new_text + " "
            new_text=new_text+wo
    retu=np.char.strip(new_text)
    return retu

def tonumbers(datta):
  nine="9"
  eight="8"
  seven="7"
  six="6"
  five="5"
  four="4"
  three="3"
  two="2"
  one="1"
  zero="0"
  varnine=" nine "
  vareight=" eight "
  varseven=" seven "
  varsix=" six "
  varfive=" five "
  varfour=" four "
  varthree=" three "
  vartwo=" two "
  varone=" one "
  varzero=" zero "
  datta = np.char.replace(datta, nine, varnine)
  datta = np.char.replace(datta, eight, vareight)
  datta = np.char.replace(datta, seven, varseven)
  datta = np.char.replace(datta, six, varsix)
  datta = np.char.replace(datta, five, varfive)
  datta = np.char.replace(datta, four, varfour)
  datta = np.char.replace(datta, three, varthree)
  datta = np.char.replace(datta, two, vartwo)
  datta = np.char.replace(datta, one, varone)
  datta = np.char.replace(datta, zero, varzero)
  return datta

def stemming(datta):
    stemm= PorterStemmer()
    temp=str(datta)
    tokens = word_tokenize(temp)
    var=""
    new_text = var
    for w in tokens:
        space=" "
        new_text = new_text + space
        new_text=new_text+stemm.stem(w)
    retu=np.char.strip(new_text)
    return retu

def preprocess(datta, query):
    if not query:
        datta = remove_header(datta)        
    datta = lowercase(datta)
    datta = tonumbers(datta)
    datta = punctuation(datta) #remove comma seperately
    datta = apostrophe(datta)
    datta = singlecharacters(datta)
    datta = stemming(datta)
    return datta

"""# Generating Postings"""

p1=pd.DataFrame()
#print(p1)
f= pd.DataFrame()
w=0
w1=1
for path in paths:
    file = open(path, 'r', encoding='cp1250')
    doc=os.path.basename(path)
    print(doc)
    text=file.read()
    t1=text.strip()
    file.close()
    preprocessed_text = preprocess(t1, False)
    # if doc%100 == 0:
    #     print(doc)
    q2=str(preprocessed_text)
    tokens = word_tokenize(q2)
    pos=w
    p2=pos
    for token in tokens:
        if token in p1:
          m1=p1[token][0]            
          p=m1
          k = [a[0] for a in p]
          if doc in k:
            for a in p:
              gh=a[0]
              gh1=a[1]
              if gh==doc:
                gh1.add(p2)
          else:
                p.append([doc,{p2}])
                f[token][0] =f[token][0]+w
        else:
            d=doc
            po=p2
            p1.insert(value=[[[d, {po}]]], loc=w, column=token)
            f.insert(value=[1], loc=w, column=token)
        p2=p2+1
        #p2=pos

p1

p1.to_pickle(title + "_positional_postings")

p1 = pd.read_pickle(title + "_positional_postings")

f.to_pickle(title + "_positional_frequency")

f = pd.read_pickle(title + "_positional_frequency")

"""# Search Query"""

def get_word_postings(word):
    f=word
    preprocessed_word = str(preprocess(f, True))
    d1=preprocessed_word
    #print(preprocessed_word)
    a=0
    d=1
    print("frequency:",f[d1][a])
    print("postings lists:",p1[d1][a])

def get_positions(posting_values, doc):
    for posting_value in posting_values:
        a1=posting_value[0]
        a2=posting_value[1]
        if a1==doc:
            return a2
    return {}

def gen_init_set_matchings(word):
    init=[]
    k1=[]
    k1=init
    d1=0
    d2=1
    word_postings = p1[word][d1]
    wo=word_postings
    for word_posting in wo:
        for positions in word_posting[d2]:
            #kl=positions
            k1.append((word_posting[d1], positions))
    return k1

def match_positional_index(init,b):
  matched_docs = []
  k1=[]
  k2=init
  w=0
  w1=1
  w3=1 
  b2=len(b)
  for p in k2:
        doc = p[w]
        pos = p[w1]

        count =w1-w3
        c=count
        for k in b:
          pos = pos+w1
          po=pos
          k_pos = p1[k][w]
          ko=k_pos
          docs_list = [z[w] for z in ko]
          if doc in docs_list:
            dl=doc
            doc_positions = get_positions(k_pos, dl)
            if pos in doc_positions:
              c=c+w3
            else:
              c=c+w3
              break
          if c==b2:
            matched_docs.append(p[0])
            k1=matched_docs
  m=set(k1)
  return m

def run_query(query):
    q1=query
    processed_query=preprocess(q1, True)
    pre=processed_query
    print(pre)
    que=str(pre)
    query_tokens = word_tokenize(que)
    print(query_tokens)
    qu=len(query_tokens)
    qt=query_tokens[0]
    w=1
    w1=0
    if qu==w:
        print("Total Document Mathces", [a[w1] for a in p1[q1][w1]])
        return [a[w1] for a in p1[q1][w1]]
    ui=qt
    gm1=ui
    init_matches=gen_init_set_matchings(gm1)
    print('vikas',init_matches)
    query_tokens.pop(w1)
    total_matched_docs= match_positional_index(init_matches, query_tokens)
    #total_matched_docs=tl
    #print('Length of Total Matched Documents')
    mk=1
    hop=2
    khj=0
    print("Matches:", total_matched_docs)
    return total_matched_docs

query ="On the day of the hearing"
#query =

lists = run_query(query)

def ru1(query):
    processed_query = preprocess(query, True)
    p1=processed_query
    d2=str(p1)
    #print(processed_query)
    query_tokens = word_tokenize(str(p1))
    q1=query_tokens
    return q1

def jaccard(query, document):
  d1=set(query)
  d2=set(document)
  inte=d1.intersection(d2)
  un=d1.union(d2)
  l1=len(inte)
  l2=len(un)
  return l1/l2

k1={}
#k= [[],[]]
d1={}
import numpy as np 
import operator
#from itertools import islice
#n = 3
#list(islice(d.items(),n))
i=0
import collections
for path in paths:
  file = open(path, 'r', encoding='cp1250')
  doc=os.path.basename(path)
  f1=file.read()
  text=f1.strip()
  file.close()
  #d1=str(preprocessed_text)
  preprocessed_text = preprocess(text, False)
  d1=str(preprocessed_text)
  tokens = word_tokenize(d1)
  query1=ru1(query)
  #tokens = word_tokenize(str(preprocessed_text))
  si=jaccard(query1,tokens)
  #print(si,doc)
  #if i==10:
    #break
  #i+=1
  k1[doc]=si 
#d1=dict(sorted(k1.items(),key=operator.itemgetter(1),reverse=True))
d1=sorted(k1.items(), key=lambda x: x[1], reverse=True)
print("Top 5 documents are:",d1[:5])
#n = 5
#print(d1)

#hj=list(islice(d.items(),5))
#print(hj)
#for w in sorted(k1, key=k1.get, reverse=True):
  #d1[w]=k1[w]
  #d1.update(w)
#print(d1)

#Total Document Matches: {'aminegg.txt'}
#for path in paths:
    #file = open(path, 'r', encoding='cp1250')
    #doc=os.path.basename(path)
    #text = file.read().strip()
    #file.close()
    #preprocessed_text = preprocess(text, False)  
    #h=jaccard_similarity(query,list(preprocessed_text))
#print(h)

#[('sqzply.txt', 0.07142857142857142), ('the-tree.txt', 0.06896551724137931), ('mike.txt', 0.046511627906976744), ('life.txt', 0.04477611940298507), ('quarter.c16', 0.0410958904109589)]
#time: 1min 6s (started: 2021-04-12 08:41:14 +00:00)